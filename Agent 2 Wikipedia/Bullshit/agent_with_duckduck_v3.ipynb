{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "058ca1c4",
   "metadata": {},
   "source": [
    "Igual que el v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68fca549",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the data available for the model\n",
    "metal_data = {\n",
    "    \"unit\":\"gram\",\n",
    "    \"currency\": \"USD\",\n",
    "    \"prices\": {\n",
    "        \"gold\": 88.1553,\n",
    "        \"silver\": 1.0523,\n",
    "        \"platinum\": 32.169,\n",
    "        \"palladium\": 35.8252,\n",
    "        \"copper\": 0.0098,\n",
    "        \"aluminum\": 0.0026,\n",
    "        \"lead\": 0.0021,\n",
    "        \"nickel\": 0.0159,\n",
    "        \"zinc\": 0.0031,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aecf1480",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "import requests\n",
    "\n",
    "# Define the tools for the agent to use, it is necessary to specify that each function is a tool\n",
    "@tool\n",
    "def get_metal_price(metal_name: str) -> str:\n",
    "    \"\"\"Fetches the current per gram in USD price of the specified metal.\n",
    "\n",
    "    Args:\n",
    "        metal_name : The name of the metal (e.g., 'gold', 'silver', 'platinum').\n",
    "\n",
    "    Returns:\n",
    "        float: The current price of the metal in dollars per gram.\n",
    "\n",
    "    Raises:\n",
    "        KeyError: If the specified metal is not found in the data source.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        metal_name = metal_name.lower().strip()\n",
    "        prices = metal_data[\"prices\"]\n",
    "        currency = metal_data[\"currency\"]\n",
    "        unit=metal_data[\"unit\"]\n",
    "        if metal_name not in prices:\n",
    "            raise KeyError(\n",
    "                f\"Metal {metal_name} not found. Available metals: {', '.join(metal_data['prices'].keys())}\"\n",
    "            )\n",
    "        price=prices[metal_name]\n",
    "        return f\"The current price of {metal_name} is {price} {currency} per {unit}.\"\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Error fetching metal price: {str(e)}\")\n",
    "    \n",
    "@tool    \n",
    "def get_currency_exchange(base: str, target: str) -> str:\n",
    "    \"\"\"\n",
    "    Returns the exchange rate from base currency to target currency.\n",
    "\n",
    "    Args:\n",
    "        base (str): The base currency (e.g., 'USD').\n",
    "        target (str): The target currency (e.g., 'EUR').\n",
    "\n",
    "    Returns:\n",
    "        str: A human-readable string showing the exchange rate,\n",
    "             or an error message if the pair is not found.\n",
    "    \"\"\"\n",
    "    fake_rates = {\n",
    "        (\"usd\", \"eur\"): 0.8,\n",
    "        (\"eur\", \"usd\"): 1.8,\n",
    "        (\"usd\", \"gbp\"): 0.7\n",
    "    }\n",
    "    rate = fake_rates.get((base.lower(), target.lower()))\n",
    "    if rate is None:\n",
    "        return f\"No exchange rate found for {base.upper()} to {target.upper()}\"\n",
    "    return  f\"{base.upper()} = {rate} {target.upper()}\"\n",
    "\n",
    "from langchain_core.tools import tool\n",
    "import requests\n",
    "\n",
    "@tool\n",
    "def get_summary_of(person: str) -> str:\n",
    "    \"\"\"\n",
    "    Returns a short summary of a famous person from Wikipedia.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        title = person.replace(\" \", \"_\")\n",
    "        url = f\"https://en.wikipedia.org/api/rest_v1/page/summary/{title}\"\n",
    "        response = requests.get(url, timeout=10,verify=False)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "\n",
    "        return data.get(\"extract\", \"No summary available.\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "\n",
    "@tool\n",
    "def get_current_president_of(country: str) -> str:\n",
    "    \"\"\"\n",
    "    Returns the name of the current president of a given country using Wikidata.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Step 1: Get Wikidata ID for the country\n",
    "        search_url = \"https://www.wikidata.org/w/api.php\"\n",
    "        search_params = {\n",
    "            \"action\": \"wbsearchentities\",\n",
    "            \"search\": country,\n",
    "            \"language\": \"en\",\n",
    "            \"format\": \"json\",\n",
    "            \"type\": \"item\"\n",
    "        }\n",
    "        search_resp = requests.get(search_url, params=search_params, timeout=10,verify=False)\n",
    "        search_resp.raise_for_status()\n",
    "        search_data = search_resp.json()\n",
    "        entity_id = search_data[\"search\"][0][\"id\"]\n",
    "\n",
    "        # Step 2: Query president (P35) of the country entity\n",
    "        sparql_url = \"https://query.wikidata.org/sparql\"\n",
    "        query = f\"\"\"\n",
    "        SELECT ?presidentLabel WHERE {{\n",
    "          wd:{entity_id} wdt:P35 ?president.\n",
    "          SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"en\". }}\n",
    "        }}\n",
    "        \"\"\"\n",
    "        headers = {\"Accept\": \"application/sparql-results+json\"}\n",
    "        sparql_resp = requests.get(sparql_url, params={\"query\": query}, headers=headers, timeout=10,verify=False)\n",
    "        sparql_resp.raise_for_status()\n",
    "        result = sparql_resp.json()\n",
    "        bindings = result[\"results\"][\"bindings\"]\n",
    "\n",
    "        if not bindings:\n",
    "            return \"President not found.\"\n",
    "        return bindings[0][\"presidentLabel\"][\"value\"]\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d39daae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama.chat_models import ChatOllama\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "# Instanciamos el modelo LLM local usando Ollama (Llama 3.2)\n",
    "llm = ChatOllama(\n",
    "    model=\"llama3.2\",   # Usamos el modelo Llama 3.2 local\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "# Vinculamos nuestra herramienta al LLM\n",
    "tools = [get_metal_price,get_currency_exchange,get_current_president_of,get_summary_of]\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "agent = create_react_agent(\n",
    "    model=llm_with_tools,\n",
    "    tools=[get_metal_price, get_currency_exchange,get_current_president_of,get_summary_of],\n",
    "    prompt = \"\"\"\n",
    "You are a ReAct agent.\n",
    "\n",
    "You do NOT know the answer to any question before using tools.\n",
    "\n",
    "For questions about metal prices like \"What is the price of METAL in CUR?\":\n",
    "1. Call the tool `get_metal_price` with the metal name (e.g., \"gold\").\n",
    "2. Then call `get_currency_exchange` with base='USD' and target=CUR.\n",
    "3. Combine both results in a final response with the price converted.\n",
    "\n",
    "For questions like \"Who is the president of COUNTRY?\":\n",
    "1. Call `get_current_president_of` with the country name.\n",
    "2. Use the result directly as the final answer.\n",
    "3. If the result is not a person’s name or indicates failure, respond with:\n",
    "   \"I could not find that information in the retrieved source.\"\n",
    "\n",
    "For questions like \"Give me a summary of PERSON\":\n",
    "1. Call `get_summary_of` with the person's name.\n",
    "2. Return the extract from the tool as your final answer.\n",
    "\n",
    "Never invent answers. Use only information provided by the tools.\n",
    "\"\"\"\n",
    "\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ae8959c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.graph import END\n",
    "from langchain_core.messages import AnyMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "from langchain.schema.messages import AIMessage,ToolMessage,HumanMessage\n",
    "\n",
    "# 1) State\n",
    "class GraphState(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], add_messages]\n",
    "\n",
    "def assistant(state: GraphState):\n",
    "    result = agent.invoke({\"messages\": state[\"messages\"]})\n",
    "    new_msgs = result[\"messages\"]\n",
    "    return {\"messages\": state[\"messages\"] + new_msgs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6dd42ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import START, END, StateGraph\n",
    "def verify_next(state: GraphState):\n",
    "    last = state[\"messages\"][-1]\n",
    "    return END if isinstance(last, AIMessage) and not getattr(last, \"tool_calls\", None) else \"assistant\"\n",
    "\n",
    "builder = StateGraph(GraphState)\n",
    "builder.add_node(\"assistant\", assistant)\n",
    "\n",
    "builder.add_edge(START, \"assistant\")                         # arranque\n",
    "builder.add_edge(\"assistant\", END)                          # siempre pasar por verify\n",
    "\n",
    "react_graph = builder.compile()\n",
    "from IPython.display import Image, display\n",
    "#display(Image(react_graph.get_graph(xray=True).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5252284f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.messages import AIMessage,ToolMessage,HumanMessage\n",
    "from copy import deepcopy\n",
    "import json\n",
    "import uuid\n",
    "\n",
    "def normalize_args(args):\n",
    "    return {k.lower(): str(v).lower() for k, v in args.items()}\n",
    "\n",
    "def fix_tool_calls_for_openai_format(messages):\n",
    "    final_messages = []\n",
    "    tool_message_buffer = {}\n",
    "    used_tool_call_ids = set()\n",
    "\n",
    "    # Indexar ToolMessages por tool_call_id\n",
    "    for msg in messages:\n",
    "        if isinstance(msg, ToolMessage):\n",
    "            tool_message_buffer[msg.tool_call_id] = msg\n",
    "\n",
    "    for msg in messages:\n",
    "        if isinstance(msg, HumanMessage):\n",
    "            final_messages.append(msg)\n",
    "\n",
    "        elif isinstance(msg, AIMessage) and msg.tool_calls and len(msg.tool_calls) > 1:\n",
    "            for tool_call in msg.tool_calls:\n",
    "                # Normalizar los args aquí\n",
    "                norm_args = normalize_args(tool_call[\"args\"])\n",
    "\n",
    "                new_msg = deepcopy(msg)\n",
    "                new_msg.tool_calls = [{\n",
    "                    \"name\": tool_call[\"name\"],\n",
    "                    \"args\": norm_args,\n",
    "                    \"id\": tool_call.get(\"id\", f\"call_{uuid.uuid4().hex[:24]}\"),\n",
    "                    \"type\": \"tool_call\"\n",
    "                }]\n",
    "                new_msg.additional_kwargs[\"tool_calls\"] = [{\n",
    "                    \"id\": tool_call.get(\"id\", f\"call_{uuid.uuid4().hex[:24]}\"),\n",
    "                    \"type\": \"function\",\n",
    "                    \"function\": {\n",
    "                        \"name\": tool_call[\"name\"],\n",
    "                        \"arguments\": json.dumps(norm_args)\n",
    "                    }\n",
    "                }]\n",
    "                final_messages.append(new_msg)\n",
    "\n",
    "                tool_msg = tool_message_buffer.get(tool_call[\"id\"])\n",
    "                if tool_msg:\n",
    "                    final_messages.append(tool_msg)\n",
    "                    used_tool_call_ids.add(tool_call[\"id\"])\n",
    "\n",
    "        elif isinstance(msg, AIMessage) and msg.tool_calls:\n",
    "            tool_call = msg.tool_calls[0]\n",
    "            norm_args = normalize_args(tool_call[\"args\"])\n",
    "\n",
    "            msg.tool_calls[0][\"args\"] = norm_args\n",
    "            msg.additional_kwargs[\"tool_calls\"] = [{\n",
    "                \"id\": tool_call.get(\"id\", f\"call_{uuid.uuid4().hex[:24]}\"),\n",
    "                \"type\": \"function\",\n",
    "                \"function\": {\n",
    "                    \"name\": tool_call[\"name\"],\n",
    "                    \"arguments\": json.dumps(norm_args)\n",
    "                }\n",
    "            }]\n",
    "            final_messages.append(msg)\n",
    "\n",
    "        elif isinstance(msg, AIMessage):\n",
    "            final_messages.append(msg)\n",
    "\n",
    "        elif isinstance(msg, ToolMessage):\n",
    "            if msg.tool_call_id not in used_tool_call_ids:\n",
    "                final_messages.append(msg)\n",
    "\n",
    "    return final_messages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84d7d8bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\eguzk\\anaconda3\\envs\\TFM\\Lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'www.wikidata.org'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "c:\\Users\\eguzk\\anaconda3\\envs\\TFM\\Lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'query.wikidata.org'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='Who is the president of France?', additional_kwargs={}, response_metadata={}, id='5577fa39-5638-489d-a0bf-349b8eb3f6f5'),\n",
       "  AIMessage(content='', additional_kwargs={}, response_metadata={'model': 'llama3.2', 'created_at': '2025-06-03T13:38:34.8193773Z', 'done': True, 'done_reason': 'stop', 'total_duration': 19896977000, 'load_duration': 3497870800, 'prompt_eval_count': 683, 'prompt_eval_duration': 14451717800, 'eval_count': 20, 'eval_duration': 1937311700, 'model_name': 'llama3.2'}, id='run--fd22aeee-ad82-4f7c-8007-3bd0515d3284-0', tool_calls=[{'name': 'get_current_president_of', 'args': {'country': 'France'}, 'id': 'cb7b680c-9687-4605-8dd2-5dd8f2e13dcb', 'type': 'tool_call'}], usage_metadata={'input_tokens': 683, 'output_tokens': 20, 'total_tokens': 703}),\n",
       "  ToolMessage(content='Emmanuel Macron', name='get_current_president_of', id='fbde5d7a-adde-438d-8873-89ef83c7d22a', tool_call_id='cb7b680c-9687-4605-8dd2-5dd8f2e13dcb'),\n",
       "  AIMessage(content='The current President of France is Emmanuel Macron.', additional_kwargs={}, response_metadata={'model': 'llama3.2', 'created_at': '2025-06-03T13:38:38.7700335Z', 'done': True, 'done_reason': 'stop', 'total_duration': 1952016400, 'load_duration': 23825700, 'prompt_eval_count': 309, 'prompt_eval_duration': 1119274300, 'eval_count': 10, 'eval_duration': 806409700, 'model_name': 'llama3.2'}, id='run--1d794f9b-ecc3-4d3b-aa1d-83074c6b9948-0', usage_metadata={'input_tokens': 309, 'output_tokens': 10, 'total_tokens': 319})]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = react_graph.invoke({\"messages\": [HumanMessage(content=\"Who is the president of France?\")]})\n",
    "#result = react_graph.invoke({\"messages\": [HumanMessage(content=\"When did Emmanuel Macron become president of France?\")]})\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "611196aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Who is the president of France?', additional_kwargs={}, response_metadata={}, id='5577fa39-5638-489d-a0bf-349b8eb3f6f5'),\n",
       " AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'cb7b680c-9687-4605-8dd2-5dd8f2e13dcb', 'type': 'function', 'function': {'name': 'get_current_president_of', 'arguments': '{\"country\": \"france\"}'}}]}, response_metadata={'model': 'llama3.2', 'created_at': '2025-06-03T13:38:34.8193773Z', 'done': True, 'done_reason': 'stop', 'total_duration': 19896977000, 'load_duration': 3497870800, 'prompt_eval_count': 683, 'prompt_eval_duration': 14451717800, 'eval_count': 20, 'eval_duration': 1937311700, 'model_name': 'llama3.2'}, id='run--fd22aeee-ad82-4f7c-8007-3bd0515d3284-0', tool_calls=[{'name': 'get_current_president_of', 'args': {'country': 'france'}, 'id': 'cb7b680c-9687-4605-8dd2-5dd8f2e13dcb', 'type': 'tool_call'}], usage_metadata={'input_tokens': 683, 'output_tokens': 20, 'total_tokens': 703}),\n",
       " ToolMessage(content='Emmanuel Macron', name='get_current_president_of', id='fbde5d7a-adde-438d-8873-89ef83c7d22a', tool_call_id='cb7b680c-9687-4605-8dd2-5dd8f2e13dcb'),\n",
       " AIMessage(content='The current President of France is Emmanuel Macron.', additional_kwargs={}, response_metadata={'model': 'llama3.2', 'created_at': '2025-06-03T13:38:38.7700335Z', 'done': True, 'done_reason': 'stop', 'total_duration': 1952016400, 'load_duration': 23825700, 'prompt_eval_count': 309, 'prompt_eval_duration': 1119274300, 'eval_count': 10, 'eval_duration': 806409700, 'model_name': 'llama3.2'}, id='run--1d794f9b-ecc3-4d3b-aa1d-83074c6b9948-0', usage_metadata={'input_tokens': 309, 'output_tokens': 10, 'total_tokens': 319})]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ragas_trace=fix_tool_calls_for_openai_format(result[\"messages\"])\n",
    "ragas_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0415982",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\eguzk\\anaconda3\\envs\\TFM\\Lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'www.wikidata.org'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "c:\\Users\\eguzk\\anaconda3\\envs\\TFM\\Lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'query.wikidata.org'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='Describe the president of France?', additional_kwargs={}, response_metadata={}, id='c044dd04-39e0-45c4-b44f-a90957a1bc7e'),\n",
       "  AIMessage(content='', additional_kwargs={}, response_metadata={'model': 'llama3.2', 'created_at': '2025-06-03T13:38:51.2040594Z', 'done': True, 'done_reason': 'stop', 'total_duration': 12396957200, 'load_duration': 25585800, 'prompt_eval_count': 682, 'prompt_eval_duration': 10358418100, 'eval_count': 20, 'eval_duration': 2011889600, 'model_name': 'llama3.2'}, id='run--5946622d-bea0-4f5b-8004-a386204171e9-0', tool_calls=[{'name': 'get_current_president_of', 'args': {'country': 'France'}, 'id': '244bdc9d-defa-4a93-9fca-5562780e5dfd', 'type': 'tool_call'}], usage_metadata={'input_tokens': 682, 'output_tokens': 20, 'total_tokens': 702}),\n",
       "  ToolMessage(content='Emmanuel Macron', name='get_current_president_of', id='abad4bcc-9357-4630-adb3-5098226a0693', tool_call_id='244bdc9d-defa-4a93-9fca-5562780e5dfd'),\n",
       "  AIMessage(content='The current President of France is Emmanuel Macron.', additional_kwargs={}, response_metadata={'model': 'llama3.2', 'created_at': '2025-06-03T13:38:54.7655454Z', 'done': True, 'done_reason': 'stop', 'total_duration': 1982817600, 'load_duration': 19959100, 'prompt_eval_count': 308, 'prompt_eval_duration': 1030392200, 'eval_count': 10, 'eval_duration': 930881200, 'model_name': 'llama3.2'}, id='run--24ed7b29-c35f-4b1b-8f72-a0bfffab06d5-0', usage_metadata={'input_tokens': 308, 'output_tokens': 10, 'total_tokens': 318})]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = react_graph.invoke({\"messages\": [HumanMessage(content=\"Describe the president of France?\")]})\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5682707e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Describe the president of France?', additional_kwargs={}, response_metadata={}, id='c044dd04-39e0-45c4-b44f-a90957a1bc7e'),\n",
       " AIMessage(content='', additional_kwargs={'tool_calls': [{'id': '244bdc9d-defa-4a93-9fca-5562780e5dfd', 'type': 'function', 'function': {'name': 'get_current_president_of', 'arguments': '{\"country\": \"france\"}'}}]}, response_metadata={'model': 'llama3.2', 'created_at': '2025-06-03T13:38:51.2040594Z', 'done': True, 'done_reason': 'stop', 'total_duration': 12396957200, 'load_duration': 25585800, 'prompt_eval_count': 682, 'prompt_eval_duration': 10358418100, 'eval_count': 20, 'eval_duration': 2011889600, 'model_name': 'llama3.2'}, id='run--5946622d-bea0-4f5b-8004-a386204171e9-0', tool_calls=[{'name': 'get_current_president_of', 'args': {'country': 'france'}, 'id': '244bdc9d-defa-4a93-9fca-5562780e5dfd', 'type': 'tool_call'}], usage_metadata={'input_tokens': 682, 'output_tokens': 20, 'total_tokens': 702}),\n",
       " ToolMessage(content='Emmanuel Macron', name='get_current_president_of', id='abad4bcc-9357-4630-adb3-5098226a0693', tool_call_id='244bdc9d-defa-4a93-9fca-5562780e5dfd'),\n",
       " AIMessage(content='The current President of France is Emmanuel Macron.', additional_kwargs={}, response_metadata={'model': 'llama3.2', 'created_at': '2025-06-03T13:38:54.7655454Z', 'done': True, 'done_reason': 'stop', 'total_duration': 1982817600, 'load_duration': 19959100, 'prompt_eval_count': 308, 'prompt_eval_duration': 1030392200, 'eval_count': 10, 'eval_duration': 930881200, 'model_name': 'llama3.2'}, id='run--24ed7b29-c35f-4b1b-8f72-a0bfffab06d5-0', usage_metadata={'input_tokens': 308, 'output_tokens': 10, 'total_tokens': 318})]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ragas_trace=fix_tool_calls_for_openai_format(result[\"messages\"])\n",
    "ragas_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b4b4d900",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\eguzk\\anaconda3\\envs\\TFM\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SingleTurnSample(user_input='Describe the president of France?', retrieved_contexts=['Emmanuel Macron'], reference_contexts=None, response='The current President of France is Emmanuel Macron.', multi_responses=None, reference='Emmanuel Macron', rubrics=None)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ragas.dataset_schema import SingleTurnSample\n",
    "from langchain_core.messages import HumanMessage, AIMessage, ToolMessage\n",
    "\n",
    "# convierte los mensajes de LangChain a SingleTurnSample\n",
    "def lc_to_ragas_sample(lc_msgs, reference: str) -> SingleTurnSample:\n",
    "    question = next(m.content for m in lc_msgs if isinstance(m, HumanMessage))\n",
    "    answer   = next(\n",
    "        m.content for m in reversed(lc_msgs)\n",
    "        if isinstance(m, AIMessage) and not getattr(m, \"tool_calls\", None)\n",
    "    )\n",
    "    contexts = [m.content for m in lc_msgs if isinstance(m, ToolMessage)]\n",
    "\n",
    "    return SingleTurnSample(\n",
    "        user_input=question,\n",
    "        response=answer,\n",
    "        retrieved_contexts=contexts,\n",
    "        reference= reference\n",
    "           )\n",
    "\n",
    "# historial de tu agente (por ejemplo result[\"messages\"])\n",
    "ragas_sample = lc_to_ragas_sample(result[\"messages\"],\"Emmanuel Macron\")\n",
    "ragas_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b33311b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EvaluationDataset(features=['user_input', 'retrieved_contexts', 'response', 'reference'], len=1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ragas import EvaluationDataset\n",
    "\n",
    "ragas_dataset = EvaluationDataset.from_list(\n",
    "    [ragas_sample.model_dump()]          # ← convertir a dict\n",
    ")\n",
    "ragas_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16b5361",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97457515",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 3/3 [04:06<00:00, 82.29s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>retrieved_contexts</th>\n",
       "      <th>response</th>\n",
       "      <th>reference</th>\n",
       "      <th>answer_relevancy</th>\n",
       "      <th>faithfulness</th>\n",
       "      <th>context_precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Describe the president of France?</td>\n",
       "      <td>[Emmanuel Macron]</td>\n",
       "      <td>The current President of France is Emmanuel Ma...</td>\n",
       "      <td>Emmanuel Macron</td>\n",
       "      <td>0.84697</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          user_input retrieved_contexts  \\\n",
       "0  Describe the president of France?  [Emmanuel Macron]   \n",
       "\n",
       "                                            response        reference  \\\n",
       "0  The current President of France is Emmanuel Ma...  Emmanuel Macron   \n",
       "\n",
       "   answer_relevancy  faithfulness  context_precision  \n",
       "0           0.84697           1.0                1.0  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ragas.evaluation import evaluate\n",
    "from ragas.metrics import   AnswerRelevancy,ContextPrecision,Faithfulness\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.llms import Ollama\n",
    "from ragas import RunConfig\n",
    "\n",
    "\n",
    "# Cargar modelo de embeddings open-source\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Envolver el modelo local\n",
    "local_llm = Ollama(model=\"mistral\", temperature=0,timeout=60000)\n",
    "wrapped_llm = LangchainLLMWrapper(local_llm)\n",
    "\n",
    "metrics = [AnswerRelevancy(),Faithfulness(), ContextPrecision()]\n",
    "#metrics = [Faithfulness()]\n",
    "\n",
    "\n",
    "#faithfulness needs ['retrieved_contexts']\n",
    "#ContextPrecision needs reference\n",
    "\n",
    "run_cfg = RunConfig(\n",
    "    timeout=3000,      # 5 minutos por job\n",
    "    max_retries=1,    # no reintentes eternamente si falla\n",
    "    max_workers=1     # todo secuencial → nada de procesos paralelos que saturen la RAM/CPU\n",
    ")\n",
    "\n",
    "# Ejecutar la evaluación\n",
    "result_eval = evaluate(\n",
    "    ragas_dataset,\n",
    "    metrics=metrics,\n",
    "    llm=wrapped_llm,\n",
    "    embeddings=embeddings,\n",
    "    batch_size=1,\n",
    "    raise_exceptions=True,   # para ver el trace completo si algo peta\n",
    "    run_config=run_cfg\n",
    ")\n",
    "#0.954384\t1.0\t1.0\n",
    "# Visualizar resultados\n",
    "result_eval.to_pandas()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TFM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
