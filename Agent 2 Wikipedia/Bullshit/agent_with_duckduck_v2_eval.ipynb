{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68fca549",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the data available for the model\n",
    "metal_data = {\n",
    "    \"unit\":\"gram\",\n",
    "    \"currency\": \"USD\",\n",
    "    \"prices\": {\n",
    "        \"gold\": 88.1553,\n",
    "        \"silver\": 1.0523,\n",
    "        \"platinum\": 32.169,\n",
    "        \"palladium\": 35.8252,\n",
    "        \"copper\": 0.0098,\n",
    "        \"aluminum\": 0.0026,\n",
    "        \"lead\": 0.0021,\n",
    "        \"nickel\": 0.0159,\n",
    "        \"zinc\": 0.0031,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aecf1480",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@tool\\ndef search_duckduckgo(query: str) -> str:\\n\\n    #Performs a DuckDuckGo search using a JSON endpoint and returns the first result snippet.\\n\\n    try:\\n        url = \"https://api.duckduckgo.com/\"\\n        params = {\"q\": query, \"format\": \"json\", \"no_html\": 1, \"skip_disambig\": 1}\\n        resp = requests.get(url, params=params, timeout=10)\\n        data = resp.json()\\n\\n        if \"AbstractText\" in data and data[\"AbstractText\"]:\\n            return data[\"AbstractText\"]\\n        elif data.get(\"RelatedTopics\"):\\n            for topic in data[\"RelatedTopics\"]:\\n                if isinstance(topic, dict) and topic.get(\"Text\"):\\n                    return topic[\"Text\"]\\n        return \"No relevant results found.\"\\n    except Exception as e:\\n        return f\"Search error: {e}\"\\nimport requests'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.tools import tool\n",
    "import requests\n",
    "\n",
    "# Define the tools for the agent to use, it is necessary to specify that each function is a tool\n",
    "@tool\n",
    "def get_metal_price(metal_name: str) -> str:\n",
    "    \"\"\"Fetches the current per gram in USD price of the specified metal.\n",
    "\n",
    "    Args:\n",
    "        metal_name : The name of the metal (e.g., 'gold', 'silver', 'platinum').\n",
    "\n",
    "    Returns:\n",
    "        float: The current price of the metal in dollars per gram.\n",
    "\n",
    "    Raises:\n",
    "        KeyError: If the specified metal is not found in the data source.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        metal_name = metal_name.lower().strip()\n",
    "        prices = metal_data[\"prices\"]\n",
    "        currency = metal_data[\"currency\"]\n",
    "        unit=metal_data[\"unit\"]\n",
    "        if metal_name not in prices:\n",
    "            raise KeyError(\n",
    "                f\"Metal {metal_name} not found. Available metals: {', '.join(metal_data['prices'].keys())}\"\n",
    "            )\n",
    "        price=prices[metal_name]\n",
    "        return f\"The current price of {metal_name} is {price} {currency} per {unit}.\"\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Error fetching metal price: {str(e)}\")\n",
    "    \n",
    "@tool    \n",
    "def get_currency_exchange(base: str, target: str) -> str:\n",
    "    \"\"\"\n",
    "    Returns the exchange rate from base currency to target currency.\n",
    "\n",
    "    Args:\n",
    "        base (str): The base currency (e.g., 'USD').\n",
    "        target (str): The target currency (e.g., 'EUR').\n",
    "\n",
    "    Returns:\n",
    "        str: A human-readable string showing the exchange rate,\n",
    "             or an error message if the pair is not found.\n",
    "    \"\"\"\n",
    "    fake_rates = {\n",
    "        (\"usd\", \"eur\"): 0.8,\n",
    "        (\"eur\", \"usd\"): 1.8,\n",
    "        (\"usd\", \"gbp\"): 0.7\n",
    "    }\n",
    "    rate = fake_rates.get((base.lower(), target.lower()))\n",
    "    if rate is None:\n",
    "        return f\"No exchange rate found for {base.upper()} to {target.upper()}\"\n",
    "    return  f\"{base.upper()} = {rate} {target.upper()}\"\n",
    "\n",
    "@tool\n",
    "def search_wikipedia(query: str) -> str:\n",
    "    \"\"\"Search Wikipedia for a summary related to the query.\"\"\"\n",
    "    url = f\"https://en.wikipedia.org/api/rest_v1/page/summary/{query.replace(' ', '_')}\"\n",
    "    response = requests.get(url, verify=False)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        return data.get(\"extract\", \"No summary found.\")\n",
    "    else:\n",
    "        return \"No relevant results found.\"\n",
    "\n",
    "\"\"\"@tool\n",
    "def search_duckduckgo(query: str) -> str:\n",
    "    \n",
    "    #Performs a DuckDuckGo search using a JSON endpoint and returns the first result snippet.\n",
    "    \n",
    "    try:\n",
    "        url = \"https://api.duckduckgo.com/\"\n",
    "        params = {\"q\": query, \"format\": \"json\", \"no_html\": 1, \"skip_disambig\": 1}\n",
    "        resp = requests.get(url, params=params, timeout=10)\n",
    "        data = resp.json()\n",
    "\n",
    "        if \"AbstractText\" in data and data[\"AbstractText\"]:\n",
    "            return data[\"AbstractText\"]\n",
    "        elif data.get(\"RelatedTopics\"):\n",
    "            for topic in data[\"RelatedTopics\"]:\n",
    "                if isinstance(topic, dict) and topic.get(\"Text\"):\n",
    "                    return topic[\"Text\"]\n",
    "        return \"No relevant results found.\"\n",
    "    except Exception as e:\n",
    "        return f\"Search error: {e}\"\n",
    "import requests\"\"\"\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d39daae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama.chat_models import ChatOllama\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "# Instanciamos el modelo LLM local usando Ollama (Llama 3.2)\n",
    "llm = ChatOllama(\n",
    "    model=\"llama3.2\",   # Usamos el modelo Llama 3.2 local\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "# Vinculamos nuestra herramienta al LLM\n",
    "tools = [get_metal_price,get_currency_exchange,search_wikipedia]\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "agent = create_react_agent(\n",
    "    model=llm_with_tools,\n",
    "    tools=[get_metal_price, get_currency_exchange,search_wikipedia],\n",
    "    prompt=\"\"\"\n",
    "You are a ReAct agent.\n",
    "\n",
    "For questions about metal prices like \"What is the price of METAL in CUR?\":\n",
    "1) Call `get_metal_price` with the metal name (e.g., \"gold\").\n",
    "2) Then call `get_currency_exchange` with base='USD' and target=CUR.\n",
    "3) Combine both results in a final response with the price converted.\n",
    "\n",
    "For general knowledge or factual questions (e.g., \"Who is the president of France?\"):\n",
    "1) Call `search_wikipedia` with a well-formed query.\n",
    "2) Use the result to respond clearly and concisely.\n",
    "\n",
    "If the tool result does not explicitly contain the answer, say \"I could not find that information in the retrieved source.\"\n",
    "Do not add content from your own knowledge.\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ae8959c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.graph import END\n",
    "from langchain_core.messages import AnyMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "from langchain.schema.messages import AIMessage,ToolMessage,HumanMessage\n",
    "\n",
    "# 1) State\n",
    "class GraphState(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], add_messages]\n",
    "\n",
    "def assistant(state: GraphState):\n",
    "    result = agent.invoke({\"messages\": state[\"messages\"]})\n",
    "    new_msgs = result[\"messages\"]\n",
    "    return {\"messages\": state[\"messages\"] + new_msgs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6dd42ec5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langgraph.graph import START, END, StateGraph\n",
    "def verify_next(state: GraphState):\n",
    "    last = state[\"messages\"][-1]\n",
    "    return END if isinstance(last, AIMessage) and not getattr(last, \"tool_calls\", None) else \"assistant\"\n",
    "\n",
    "builder = StateGraph(GraphState)\n",
    "builder.add_node(\"assistant\", assistant)\n",
    "\n",
    "builder.add_edge(START, \"assistant\")                         # arranque\n",
    "builder.add_edge(\"assistant\", END)                          # siempre pasar por verify\n",
    "\n",
    "react_graph = builder.compile()\n",
    "from IPython.display import Image, display\n",
    "display(Image(react_graph.get_graph(xray=True).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5252284f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.messages import AIMessage,ToolMessage,HumanMessage\n",
    "from copy import deepcopy\n",
    "import json\n",
    "import uuid\n",
    "\n",
    "def normalize_args(args):\n",
    "    return {k.lower(): str(v).lower() for k, v in args.items()}\n",
    "\n",
    "def fix_tool_calls_for_openai_format(messages):\n",
    "    final_messages = []\n",
    "    tool_message_buffer = {}\n",
    "    used_tool_call_ids = set()\n",
    "\n",
    "    # Indexar ToolMessages por tool_call_id\n",
    "    for msg in messages:\n",
    "        if isinstance(msg, ToolMessage):\n",
    "            tool_message_buffer[msg.tool_call_id] = msg\n",
    "\n",
    "    for msg in messages:\n",
    "        if isinstance(msg, HumanMessage):\n",
    "            final_messages.append(msg)\n",
    "\n",
    "        elif isinstance(msg, AIMessage) and msg.tool_calls and len(msg.tool_calls) > 1:\n",
    "            for tool_call in msg.tool_calls:\n",
    "                # Normalizar los args aquí\n",
    "                norm_args = normalize_args(tool_call[\"args\"])\n",
    "\n",
    "                new_msg = deepcopy(msg)\n",
    "                new_msg.tool_calls = [{\n",
    "                    \"name\": tool_call[\"name\"],\n",
    "                    \"args\": norm_args,\n",
    "                    \"id\": tool_call.get(\"id\", f\"call_{uuid.uuid4().hex[:24]}\"),\n",
    "                    \"type\": \"tool_call\"\n",
    "                }]\n",
    "                new_msg.additional_kwargs[\"tool_calls\"] = [{\n",
    "                    \"id\": tool_call.get(\"id\", f\"call_{uuid.uuid4().hex[:24]}\"),\n",
    "                    \"type\": \"function\",\n",
    "                    \"function\": {\n",
    "                        \"name\": tool_call[\"name\"],\n",
    "                        \"arguments\": json.dumps(norm_args)\n",
    "                    }\n",
    "                }]\n",
    "                final_messages.append(new_msg)\n",
    "\n",
    "                tool_msg = tool_message_buffer.get(tool_call[\"id\"])\n",
    "                if tool_msg:\n",
    "                    final_messages.append(tool_msg)\n",
    "                    used_tool_call_ids.add(tool_call[\"id\"])\n",
    "\n",
    "        elif isinstance(msg, AIMessage) and msg.tool_calls:\n",
    "            tool_call = msg.tool_calls[0]\n",
    "            norm_args = normalize_args(tool_call[\"args\"])\n",
    "\n",
    "            msg.tool_calls[0][\"args\"] = norm_args\n",
    "            msg.additional_kwargs[\"tool_calls\"] = [{\n",
    "                \"id\": tool_call.get(\"id\", f\"call_{uuid.uuid4().hex[:24]}\"),\n",
    "                \"type\": \"function\",\n",
    "                \"function\": {\n",
    "                    \"name\": tool_call[\"name\"],\n",
    "                    \"arguments\": json.dumps(norm_args)\n",
    "                }\n",
    "            }]\n",
    "            final_messages.append(msg)\n",
    "\n",
    "        elif isinstance(msg, AIMessage):\n",
    "            final_messages.append(msg)\n",
    "\n",
    "        elif isinstance(msg, ToolMessage):\n",
    "            if msg.tool_call_id not in used_tool_call_ids:\n",
    "                final_messages.append(msg)\n",
    "\n",
    "    return final_messages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84d7d8bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\eguzk\\anaconda3\\envs\\TFM\\Lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'en.wikipedia.org'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='Who is the president of France?', additional_kwargs={}, response_metadata={}, id='401468af-a415-4cfb-9a65-74739b1ba83e'),\n",
       "  AIMessage(content='', additional_kwargs={}, response_metadata={'model': 'llama3.2', 'created_at': '2025-06-02T13:38:14.7906563Z', 'done': True, 'done_reason': 'stop', 'total_duration': 13922821500, 'load_duration': 33463900, 'prompt_eval_count': 571, 'prompt_eval_duration': 12314152800, 'eval_count': 20, 'eval_duration': 1572082300, 'model_name': 'llama3.2'}, id='run--12116933-f523-41d2-80af-29e9ae770d69-0', tool_calls=[{'name': 'search_wikipedia', 'args': {'query': 'President of France'}, 'id': 'e3b9b22b-2144-455a-a726-15e01a4b9919', 'type': 'tool_call'}], usage_metadata={'input_tokens': 571, 'output_tokens': 20, 'total_tokens': 591}),\n",
       "  ToolMessage(content='The president of France, officially the president of the French Republic, is the executive head of state of France, and the commander-in-chief of the French Armed Forces. As the presidency is the supreme magistracy of the country, the position is the highest office in France. The powers, functions and duties of prior presidential offices, in addition to their relation with the prime minister and government of France, have over time differed with the various constitutional documents since the Second Republic.', name='search_wikipedia', id='606f3217-1cf3-43a6-a42b-ddf8ac50e183', tool_call_id='e3b9b22b-2144-455a-a726-15e01a4b9919'),\n",
       "  AIMessage(content='I could not find that information in the retrieved source.', additional_kwargs={}, response_metadata={'model': 'llama3.2', 'created_at': '2025-06-02T13:38:19.1095099Z', 'done': True, 'done_reason': 'stop', 'total_duration': 3559820700, 'load_duration': 43134800, 'prompt_eval_count': 344, 'prompt_eval_duration': 2655230500, 'eval_count': 12, 'eval_duration': 854371000, 'model_name': 'llama3.2'}, id='run--fbdcac5a-d573-47b4-95f4-b3d2286413e5-0', usage_metadata={'input_tokens': 344, 'output_tokens': 12, 'total_tokens': 356})]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = react_graph.invoke({\"messages\": [HumanMessage(content=\"Who is the president of France?\")]})\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1faa191",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Who is the president of France?', additional_kwargs={}, response_metadata={}, id='401468af-a415-4cfb-9a65-74739b1ba83e'),\n",
       " AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'e3b9b22b-2144-455a-a726-15e01a4b9919', 'type': 'function', 'function': {'name': 'search_wikipedia', 'arguments': '{\"query\": \"president of france\"}'}}]}, response_metadata={'model': 'llama3.2', 'created_at': '2025-06-02T13:38:14.7906563Z', 'done': True, 'done_reason': 'stop', 'total_duration': 13922821500, 'load_duration': 33463900, 'prompt_eval_count': 571, 'prompt_eval_duration': 12314152800, 'eval_count': 20, 'eval_duration': 1572082300, 'model_name': 'llama3.2'}, id='run--12116933-f523-41d2-80af-29e9ae770d69-0', tool_calls=[{'name': 'search_wikipedia', 'args': {'query': 'president of france'}, 'id': 'e3b9b22b-2144-455a-a726-15e01a4b9919', 'type': 'tool_call'}], usage_metadata={'input_tokens': 571, 'output_tokens': 20, 'total_tokens': 591}),\n",
       " ToolMessage(content='The president of France, officially the president of the French Republic, is the executive head of state of France, and the commander-in-chief of the French Armed Forces. As the presidency is the supreme magistracy of the country, the position is the highest office in France. The powers, functions and duties of prior presidential offices, in addition to their relation with the prime minister and government of France, have over time differed with the various constitutional documents since the Second Republic.', name='search_wikipedia', id='606f3217-1cf3-43a6-a42b-ddf8ac50e183', tool_call_id='e3b9b22b-2144-455a-a726-15e01a4b9919'),\n",
       " AIMessage(content='I could not find that information in the retrieved source.', additional_kwargs={}, response_metadata={'model': 'llama3.2', 'created_at': '2025-06-02T13:38:19.1095099Z', 'done': True, 'done_reason': 'stop', 'total_duration': 3559820700, 'load_duration': 43134800, 'prompt_eval_count': 344, 'prompt_eval_duration': 2655230500, 'eval_count': 12, 'eval_duration': 854371000, 'model_name': 'llama3.2'}, id='run--fbdcac5a-d573-47b4-95f4-b3d2286413e5-0', usage_metadata={'input_tokens': 344, 'output_tokens': 12, 'total_tokens': 356})]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ragas_trace=fix_tool_calls_for_openai_format(result[\"messages\"])\n",
    "ragas_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28f81e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_dataset2 = [\n",
    "    {\n",
    "        \"question\": \"Who is the president of France?\",\n",
    "        \"expected_tool_calls\": [\n",
    "            {\"name\": \"search_wikipedia\", \"args\": {\"query\": \"President of France\"}}\n",
    "        ],\n",
    "        \"expected_substring\": \"Macron\"\n",
    "    }]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4b4d900",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\eguzk\\anaconda3\\envs\\TFM\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SingleTurnSample(user_input='Who is the president of France?', retrieved_contexts=['The president of France, officially the president of the French Republic, is the executive head of state of France, and the commander-in-chief of the French Armed Forces. As the presidency is the supreme magistracy of the country, the position is the highest office in France. The powers, functions and duties of prior presidential offices, in addition to their relation with the prime minister and government of France, have over time differed with the various constitutional documents since the Second Republic.'], reference_contexts=None, response='I could not find that information in the retrieved source.', multi_responses=None, reference='Emmanuel Macron', rubrics=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ragas.dataset_schema import SingleTurnSample\n",
    "from langchain_core.messages import HumanMessage, AIMessage, ToolMessage\n",
    "\n",
    "# convierte los mensajes de LangChain a SingleTurnSample\n",
    "def lc_to_ragas_sample(lc_msgs) -> SingleTurnSample:\n",
    "    question = next(m.content for m in lc_msgs if isinstance(m, HumanMessage))\n",
    "    answer   = next(\n",
    "        m.content for m in reversed(lc_msgs)\n",
    "        if isinstance(m, AIMessage) and not getattr(m, \"tool_calls\", None)\n",
    "    )\n",
    "    contexts = [m.content for m in lc_msgs if isinstance(m, ToolMessage)]\n",
    "    retrieved_contexts = [\"The current president of France is Emmanuel Macron, who has been serving since May 2017.\"]\n",
    "\n",
    "    return SingleTurnSample(\n",
    "        user_input=question,\n",
    "        response=answer,\n",
    "        retrieved_contexts=contexts,\n",
    "        #reference_contexts= contexts,    # lista (puede estar vacía)\n",
    "        reference= \"Emmanuel Macron\"\n",
    "           )\n",
    "\n",
    "# historial de tu agente (por ejemplo result[\"messages\"])\n",
    "ragas_sample = lc_to_ragas_sample(result[\"messages\"])\n",
    "ragas_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b33311b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EvaluationDataset(features=['user_input', 'retrieved_contexts', 'response', 'reference'], len=1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ragas import EvaluationDataset\n",
    "\n",
    "ragas_dataset = EvaluationDataset.from_list(\n",
    "    [ragas_sample.model_dump()]          # ← convertir a dict\n",
    ")\n",
    "ragas_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97457515",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eguzk\\AppData\\Local\\Temp\\ipykernel_20260\\2304575410.py:8: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
      "Evaluating:  33%|███▎      | 1/3 [02:34<05:08, 154.04s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     14\u001b[39m metrics = [AnswerRelevancy(),Faithfulness(), ContextPrecision()]\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m#faithfulness needs ['retrieved_contexts']\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m#ContextPrecision needs reference\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# Ejecutar la evaluación\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m result_eval = \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43mragas_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43mllm\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwrapped_llm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m  \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#for each metric it is used the LLM sometimes more than once\u001b[39;49;00m\n\u001b[32m     24\u001b[39m \u001b[43m     \u001b[49m\u001b[38;5;66;43;03m#  (more that one prompt) so this way it is only applied one prompt each\u001b[39;49;00m\n\u001b[32m     25\u001b[39m \n\u001b[32m     26\u001b[39m \n\u001b[32m     27\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m#0.954384\t1.0\t1.0\u001b[39;00m\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# Visualizar resultados\u001b[39;00m\n\u001b[32m     30\u001b[39m result_eval.to_pandas()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\eguzk\\anaconda3\\envs\\TFM\\Lib\\site-packages\\ragas\\_analytics.py:227\u001b[39m, in \u001b[36mtrack_was_completed.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    224\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[32m    225\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args: P.args, **kwargs: P.kwargs) -> t.Any:\n\u001b[32m    226\u001b[39m     track(IsCompleteEvent(event_type=func.\u001b[34m__name__\u001b[39m, is_completed=\u001b[38;5;28;01mFalse\u001b[39;00m))\n\u001b[32m--> \u001b[39m\u001b[32m227\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    228\u001b[39m     track(IsCompleteEvent(event_type=func.\u001b[34m__name__\u001b[39m, is_completed=\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[32m    230\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\eguzk\\anaconda3\\envs\\TFM\\Lib\\site-packages\\ragas\\evaluation.py:299\u001b[39m, in \u001b[36mevaluate\u001b[39m\u001b[34m(dataset, metrics, llm, embeddings, experiment_name, callbacks, run_config, token_usage_parser, raise_exceptions, column_map, show_progress, batch_size, _run_id, _pbar)\u001b[39m\n\u001b[32m    296\u001b[39m scores: t.List[t.Dict[\u001b[38;5;28mstr\u001b[39m, t.Any]] = []\n\u001b[32m    297\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    298\u001b[39m     \u001b[38;5;66;03m# get the results\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m299\u001b[39m     results = \u001b[43mexecutor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresults\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    300\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m results == []:\n\u001b[32m    301\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m ExceptionInRunner()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\eguzk\\anaconda3\\envs\\TFM\\Lib\\site-packages\\ragas\\executor.py:213\u001b[39m, in \u001b[36mExecutor.results\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    210\u001b[39m             nest_asyncio.apply()\n\u001b[32m    211\u001b[39m             \u001b[38;5;28mself\u001b[39m._nest_asyncio_applied = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m213\u001b[39m results = \u001b[43masyncio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_process_jobs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    214\u001b[39m sorted_results = \u001b[38;5;28msorted\u001b[39m(results, key=\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[32m0\u001b[39m])\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m [r[\u001b[32m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m sorted_results]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\eguzk\\anaconda3\\envs\\TFM\\Lib\\site-packages\\nest_asyncio.py:30\u001b[39m, in \u001b[36m_patch_asyncio.<locals>.run\u001b[39m\u001b[34m(main, debug)\u001b[39m\n\u001b[32m     28\u001b[39m task = asyncio.ensure_future(main)\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     32\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m task.done():\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\eguzk\\anaconda3\\envs\\TFM\\Lib\\site-packages\\nest_asyncio.py:92\u001b[39m, in \u001b[36m_patch_loop.<locals>.run_until_complete\u001b[39m\u001b[34m(self, future)\u001b[39m\n\u001b[32m     90\u001b[39m     f._log_destroy_pending = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     91\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f.done():\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._stopping:\n\u001b[32m     94\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\eguzk\\anaconda3\\envs\\TFM\\Lib\\site-packages\\nest_asyncio.py:115\u001b[39m, in \u001b[36m_patch_loop.<locals>._run_once\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    108\u001b[39m     heappop(scheduled)\n\u001b[32m    110\u001b[39m timeout = (\n\u001b[32m    111\u001b[39m     \u001b[32m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ready \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._stopping\n\u001b[32m    112\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mmax\u001b[39m(\n\u001b[32m    113\u001b[39m         scheduled[\u001b[32m0\u001b[39m]._when - \u001b[38;5;28mself\u001b[39m.time(), \u001b[32m0\u001b[39m), \u001b[32m86400\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m scheduled\n\u001b[32m    114\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m event_list = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_selector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    116\u001b[39m \u001b[38;5;28mself\u001b[39m._process_events(event_list)\n\u001b[32m    118\u001b[39m end_time = \u001b[38;5;28mself\u001b[39m.time() + \u001b[38;5;28mself\u001b[39m._clock_resolution\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\eguzk\\anaconda3\\envs\\TFM\\Lib\\selectors.py:314\u001b[39m, in \u001b[36mSelectSelector.select\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    312\u001b[39m ready = []\n\u001b[32m    313\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m314\u001b[39m     r, w, _ = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_select\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_readers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_writers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[32m    316\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\eguzk\\anaconda3\\envs\\TFM\\Lib\\selectors.py:305\u001b[39m, in \u001b[36mSelectSelector._select\u001b[39m\u001b[34m(self, r, w, _, timeout)\u001b[39m\n\u001b[32m    304\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_select\u001b[39m(\u001b[38;5;28mself\u001b[39m, r, w, _, timeout=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m305\u001b[39m     r, w, x = \u001b[43mselect\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    306\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m r, w + x, []\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from ragas.evaluation import evaluate\n",
    "from ragas.metrics import   AnswerRelevancy,ContextPrecision,Faithfulness\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "\n",
    "# Cargar modelo de embeddings open-source\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Envolver el modelo local\n",
    "local_llm = ChatOllama(model=\"llama3.2\", temperature=0)\n",
    "wrapped_llm = LangchainLLMWrapper(local_llm)\n",
    "\n",
    "metrics = [AnswerRelevancy(),Faithfulness(), ContextPrecision()]\n",
    "#faithfulness needs ['retrieved_contexts']\n",
    "#ContextPrecision needs reference\n",
    "# Ejecutar la evaluación\n",
    "result_eval = evaluate(\n",
    "    ragas_dataset,\n",
    "    metrics=metrics,\n",
    "    llm=wrapped_llm,\n",
    "    embeddings=embeddings , \n",
    "    batch_size  = 1, #for each metric it is used the LLM sometimes more than once\n",
    "     #  (more that one prompt) so this way it is only applied one prompt each\n",
    "\n",
    "\n",
    ")\n",
    "#0.954384\t1.0\t1.0\n",
    "# Visualizar resultados\n",
    "result_eval.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03444f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.llms import LangchainLLMWrapper\n",
    "from langchain_ollama.chat_models import ChatOllama\n",
    "\n",
    "\n",
    "# ── wrapper que imprime los prompts que recibe ────────────────────────────\n",
    "class DebugWrapper(LangchainLLMWrapper):\n",
    "    async def agenerate(self, prompts, **kwargs):\n",
    "        print(\"\\n\\n🔹 PROMPTS ENVIADOS AL LLM EVALUADOR 🔹\")\n",
    "        for i, p in enumerate(prompts):\n",
    "            # muestra los 400 primeros caracteres para no saturar\n",
    "            print(f\"\\n--- prompt {i+1} ({len(p.split())} tokens) ---\\n{p[:400]}…\")\n",
    "        # llama al método original para no romper el flujo\n",
    "        return await super().agenerate(prompts, **kwargs)\n",
    "\n",
    "# usa el wrapper-debug\n",
    "wrapped_debug_llm = DebugWrapper(local_llm)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TFM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
